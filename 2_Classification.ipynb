{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 508 HW 1 Part 2: Classification\n",
    "\n",
    "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation, Bias-Variance trade-off, Overfitting\n",
    "\n",
    "In this section, we will demonstrate data splitting and the validation process in machine learning paradigms. We will use the Iris dataset from the `sklearn` library.\n",
    "\n",
    "Objective:\n",
    "- Train a Fully-Connected Network (FCN) for classification.  \n",
    "- Partition the data using three-fold cross-validation and report the training, validation, and testing accuracy.  \n",
    "- Train the model using cross-entropy loss and evaluate it with 0/1 loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries and dataset\n",
    "import numpy as np\n",
    "# load sklearn for ML functions\n",
    "from sklearn.datasets import load_iris\n",
    "# load torch dataaset for training NNs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 1**: Implement the cross validation function\n",
    "\n",
    "In this function:\n",
    "1.  Shuffle the dataset using `np.random.shuffle`.\n",
    "2.  Create partition indices using `np.linspace`.\n",
    "3.  Loop through `n_folds`:\n",
    "    *   Determine indices for `valid` set (from `partitions[i]` to `partitions[i+1]`).\n",
    "    *   Determine indices for `train` set (the rest).\n",
    "    *   Store the partitioned arrays `(x_train, y_train, x_valid, y_valid)` in `folds` list.\n",
    "4.  Return `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton for TODO 1\n",
    "def cross_validation(x: np.array, y: np.array, n_folds: int=3):\n",
    "    \"\"\"\n",
    "    Splitting the dataset to the given fold\n",
    "    Parameters:\n",
    "    - x: Feaures of the dataset, with shape (n_samples, n_features)\n",
    "    - y: Class label of the dataset, with shape (n_samples,)\n",
    "    - n_folds: the given number of partitions\n",
    "    \n",
    "    Returns:\n",
    "    - folds (list): List of tuples (x_train, y_train, x_valid, y_valid)\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    n = x.shape[0]\n",
    "    num = np.arange(0, n)\n",
    "    np.random.shuffle(num)\n",
    "    # 1. Shuffle indices\n",
    "    # ... your code here ...\n",
    "    cut = n // n_folds\n",
    "    for i in range(n_folds):\n",
    "        start = i * cut\n",
    "        end = (i + 1) * cut\n",
    "\n",
    "        idv = num[start:end]                  \n",
    "        idt = np.concatenate([num[:start],    \n",
    "                               num[end:]])\n",
    "\n",
    "        folds.append((x[idt], y[idt], x[idv], y[idv]))\n",
    "    \n",
    "\n",
    "    # 2. Divide indices into n_folds+1 partitions\n",
    "    # ... your code here ...\n",
    "    \n",
    "    # 3. Loop through folds\n",
    "    # ... your code here ...\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton for TODO 1 (continued)\n",
    "# fixed the random seed\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Load Iris dataset\n",
    "iris = load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "# 2. Split into training and validation sets using cross_validation\n",
    "# three_folds = ...\n",
    "three_folds = cross_validation(x, y, n_folds= 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 2**: Build a Fully-Connect Networks with PyTorch\n",
    "\n",
    "Define a PyTorch model `FCN_model`:\n",
    "1.  In `__init__`, define 3 fully connected layers (`nn.Linear`):\n",
    "    *   Layer 1: Input size 4 -> `n_hidden` units.\n",
    "    *   Layer 2: `n_hidden` -> `n_hidden`.\n",
    "    *   Layer 3: `n_hidden` -> 3 (output classes).\n",
    "2.  In `forward`:\n",
    "    *   Pass input through Layer 1, apply ReLU.\n",
    "    *   Pass through Layer 2, apply ReLU.\n",
    "    *   Pass through Layer 3 (return logits, no activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton for TODO 2\n",
    "class FCN_model(nn.Module):\n",
    "    # take the argument for the number of hidden units\n",
    "    def __init__(self, n_hidden=32):\n",
    "        super(FCN_model, self).__init__()\n",
    "        # 1. Define Layer 1: Linear(4, n_hidden)\n",
    "        # ... your code here ...\n",
    "        self.fc1 = nn.Linear(4, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 3)\n",
    "\n",
    "        # 2. Define Layer 2: Linear(n_hidden, n_hidden)\n",
    "        # ... your code here ...\n",
    "        \n",
    "        # 3. Define Output Layer: Linear(n_hidden, 3)\n",
    "        # ... your code here ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the evaluation and training functions for the FCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module, \n",
    "         x:torch.tensor, \n",
    "         y:torch.tensor) -> float:\n",
    "    \"\"\"Evaluate the model: inference the model with 0/1 loss\n",
    "    We can define the output label is the maximum logit from the model\n",
    "    \n",
    "    Parameters:\n",
    "    - model: the FCN model\n",
    "    - x: input features\n",
    "    - y: ground truth labels, dtype=long\n",
    "\n",
    "    Returns:\n",
    "    - loss: the average 0/1 loss value \n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(model(x), dim=1)\n",
    "\n",
    "    loss = 0\n",
    "    for y_pred, y_gt in zip(preds, y):\n",
    "        if y_pred != y_gt:\n",
    "            loss += 1\n",
    "    print(f\"Averaging 0/1 loss: {loss/preds.shape[0]:.4f}\")\n",
    "    return loss/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, \n",
    "          x_train:torch.tensor, \n",
    "          y_train:torch.tensor,\n",
    "          x_valid:torch.tensor,\n",
    "          y_valid:torch.tensor,\n",
    "          epochs:int=300):\n",
    "    \"\"\"Trining process\n",
    "    Parameters:\n",
    "    - model: the FCN model\n",
    "    - x_train, y_train: trainig features and labels (dtype=long)\n",
    "    - x_valid, y_valid: validation features and labels (dtype=long)\n",
    "    - epochs: number of the epoches for training\n",
    "    \"\"\"\n",
    "    # To simplify the process\n",
    "    # we do not take batches but use all the training samples\n",
    "    # set up the objective function and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # Forward pass\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {loss.item():.4f}\")\n",
    "            print(f\"[Train] \", end=\"\")\n",
    "            eval(model, x_train, y_train)\n",
    "            print(f\"[Valid] \", end=\"\")\n",
    "            eval(model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 3**: Conduct the training/validation process in each fold\n",
    "We will use three-fold validation.\n",
    "\n",
    "1.  Instantiate lists `train_losses` and `valid_losses`.\n",
    "2.  Loop through `three_folds`:\n",
    "    *   Instantiate `FCN_model(n_hidden=32)`.\n",
    "    *   Convert data arrays to `torch.Tensor` (and labels to `dtype=torch.long`).\n",
    "    *   Train the model for 500 epochs using `train()`.\n",
    "    *   Evaluate using `eval()` on training data and validation data, appending results to the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Fold 0 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.6993\n",
      "[Train] Averaging 0/1 loss: 0.3300\n",
      "[Valid] Averaging 0/1 loss: 0.2800\n",
      "Epoch [200/500], Cross Entropy Loss: 0.4800\n",
      "[Train] Averaging 0/1 loss: 0.0900\n",
      "[Valid] Averaging 0/1 loss: 0.0800\n",
      "Epoch [300/500], Cross Entropy Loss: 0.3720\n",
      "[Train] Averaging 0/1 loss: 0.0400\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Epoch [400/500], Cross Entropy Loss: 0.2906\n",
      "[Train] Averaging 0/1 loss: 0.0400\n",
      "[Valid] Averaging 0/1 loss: 0.0000\n",
      "Epoch [500/500], Cross Entropy Loss: 0.2283\n",
      "[Train] Averaging 0/1 loss: 0.0300\n",
      "[Valid] Averaging 0/1 loss: 0.0000\n",
      "Averaging 0/1 loss: 0.0300\n",
      "Averaging 0/1 loss: 0.0000\n",
      "===== Training Fold 1 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.7244\n",
      "[Train] Averaging 0/1 loss: 0.3300\n",
      "[Valid] Averaging 0/1 loss: 0.3400\n",
      "Epoch [200/500], Cross Entropy Loss: 0.4767\n",
      "[Train] Averaging 0/1 loss: 0.1200\n",
      "[Valid] Averaging 0/1 loss: 0.1800\n",
      "Epoch [300/500], Cross Entropy Loss: 0.3630\n",
      "[Train] Averaging 0/1 loss: 0.0300\n",
      "[Valid] Averaging 0/1 loss: 0.1200\n",
      "Epoch [400/500], Cross Entropy Loss: 0.2787\n",
      "[Train] Averaging 0/1 loss: 0.0200\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Epoch [500/500], Cross Entropy Loss: 0.2140\n",
      "[Train] Averaging 0/1 loss: 0.0200\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Averaging 0/1 loss: 0.0200\n",
      "Averaging 0/1 loss: 0.0600\n",
      "===== Training Fold 2 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.7177\n",
      "[Train] Averaging 0/1 loss: 0.3100\n",
      "[Valid] Averaging 0/1 loss: 0.3600\n",
      "Epoch [200/500], Cross Entropy Loss: 0.4794\n",
      "[Train] Averaging 0/1 loss: 0.1600\n",
      "[Valid] Averaging 0/1 loss: 0.1800\n",
      "Epoch [300/500], Cross Entropy Loss: 0.3717\n",
      "[Train] Averaging 0/1 loss: 0.0600\n",
      "[Valid] Averaging 0/1 loss: 0.0400\n",
      "Epoch [400/500], Cross Entropy Loss: 0.2951\n",
      "[Train] Averaging 0/1 loss: 0.0300\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Epoch [500/500], Cross Entropy Loss: 0.2388\n",
      "[Train] Averaging 0/1 loss: 0.0300\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Averaging 0/1 loss: 0.0300\n",
      "Averaging 0/1 loss: 0.0200\n"
     ]
    }
   ],
   "source": [
    "# Skeleton for TODO 3\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
    "    print(f\"===== Training Fold {idx} =====\")\n",
    "    model = FCN_model(n_hidden=32)\n",
    "\n",
    "    # 2. Convert numpy arrays to torch tensors\n",
    "    x_train_t = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    x_valid_t = torch.tensor(x_valid, dtype=torch.float32)\n",
    "    y_valid_t = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "    # 3. Train model (500 epochs)\n",
    "    train(\n",
    "        model=model,\n",
    "        x_train=x_train_t,\n",
    "        y_train=y_train_t,\n",
    "        x_valid=x_valid_t,\n",
    "        y_valid=y_valid_t,\n",
    "        epochs=500\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate and store results\n",
    "    train_loss = eval(model, x_train_t, y_train_t)\n",
    "    valid_loss = eval(model, x_valid_t, y_valid_t)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold, training loss, validation loss\n",
      "    0,          0.03,            0.00\n",
      "    1,          0.02,            0.06\n",
      "    2,          0.03,            0.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"#Fold, training loss, validation loss\")\n",
    "for idx, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
    "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 4**: Check over-fitting with complex model\n",
    "Repeat the procedure from TODO 3, but this time use a more complex model:\n",
    "*   Set `n_hidden` to 2048.\n",
    "*   Train for 500 epochs.\n",
    "*   Store results in `train_overfit` and `valid_overfit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Fold 0 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.3589\n",
      "[Train] Averaging 0/1 loss: 0.1900\n",
      "[Valid] Averaging 0/1 loss: 0.1600\n",
      "Epoch [200/500], Cross Entropy Loss: 0.2593\n",
      "[Train] Averaging 0/1 loss: 0.0800\n",
      "[Valid] Averaging 0/1 loss: 0.1400\n",
      "Epoch [300/500], Cross Entropy Loss: 0.1477\n",
      "[Train] Averaging 0/1 loss: 0.0600\n",
      "[Valid] Averaging 0/1 loss: 0.1000\n",
      "Epoch [400/500], Cross Entropy Loss: 0.0904\n",
      "[Train] Averaging 0/1 loss: 0.0200\n",
      "[Valid] Averaging 0/1 loss: 0.0000\n",
      "Epoch [500/500], Cross Entropy Loss: 0.0803\n",
      "[Train] Averaging 0/1 loss: 0.0200\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Averaging 0/1 loss: 0.0200\n",
      "Averaging 0/1 loss: 0.0200\n",
      "===== Training Fold 1 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.3019\n",
      "[Train] Averaging 0/1 loss: 0.1400\n",
      "[Valid] Averaging 0/1 loss: 0.0800\n",
      "Epoch [200/500], Cross Entropy Loss: 0.1435\n",
      "[Train] Averaging 0/1 loss: 0.0700\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Epoch [300/500], Cross Entropy Loss: 0.0651\n",
      "[Train] Averaging 0/1 loss: 0.0000\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Epoch [400/500], Cross Entropy Loss: 0.0486\n",
      "[Train] Averaging 0/1 loss: 0.0100\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Epoch [500/500], Cross Entropy Loss: 0.0403\n",
      "[Train] Averaging 0/1 loss: 0.0100\n",
      "[Valid] Averaging 0/1 loss: 0.0600\n",
      "Averaging 0/1 loss: 0.0100\n",
      "Averaging 0/1 loss: 0.0600\n",
      "===== Training Fold 2 =====\n",
      "Epoch [100/500], Cross Entropy Loss: 0.2977\n",
      "[Train] Averaging 0/1 loss: 0.1800\n",
      "[Valid] Averaging 0/1 loss: 0.2000\n",
      "Epoch [200/500], Cross Entropy Loss: 0.2114\n",
      "[Train] Averaging 0/1 loss: 0.1000\n",
      "[Valid] Averaging 0/1 loss: 0.0800\n",
      "Epoch [300/500], Cross Entropy Loss: 0.1494\n",
      "[Train] Averaging 0/1 loss: 0.0600\n",
      "[Valid] Averaging 0/1 loss: 0.0400\n",
      "Epoch [400/500], Cross Entropy Loss: 0.1220\n",
      "[Train] Averaging 0/1 loss: 0.0600\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Epoch [500/500], Cross Entropy Loss: 0.1092\n",
      "[Train] Averaging 0/1 loss: 0.0400\n",
      "[Valid] Averaging 0/1 loss: 0.0200\n",
      "Averaging 0/1 loss: 0.0400\n",
      "Averaging 0/1 loss: 0.0200\n"
     ]
    }
   ],
   "source": [
    "# Skeleton for TODO 4\n",
    "train_overfit, valid_overfit = [], []\n",
    "\n",
    "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
    "    print(f\"===== Training Fold {idx} =====\")\n",
    "    model = FCN_model(n_hidden=2048)\n",
    "\n",
    "    # 2. Convert to torch Tensors\n",
    "    x_train_t = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    x_valid_t = torch.tensor(x_valid, dtype=torch.float32)\n",
    "    y_valid_t = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "    # 3. Train (500 epochs)\n",
    "    train(\n",
    "        model=model,\n",
    "        x_train=x_train_t,\n",
    "        y_train=y_train_t,\n",
    "        x_valid=x_valid_t,\n",
    "        y_valid=y_valid_t,\n",
    "        epochs=500\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate and store results\n",
    "    train_loss = eval(model, x_train_t, y_train_t)\n",
    "    valid_loss = eval(model, x_valid_t, y_valid_t)\n",
    "\n",
    "    train_overfit.append(train_loss)\n",
    "    valid_overfit.append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold, training loss, validation loss\n",
      "    0,          0.02,            0.02\n",
      "    1,          0.01,            0.06\n",
      "    2,          0.04,            0.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"#Fold, training loss, validation loss\")\n",
    "for idx, (train_loss, valid_loss) in enumerate(zip(train_overfit, valid_overfit)):\n",
    "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 5**: Compare the FCN with statistical ML models\n",
    "Use `sklearn.naive_bayes.GaussianNB`:\n",
    "1.  Loop through `three_folds`.\n",
    "2.  Instantiate `GaussianNB`.\n",
    "3.  Fit the model on training data.\n",
    "4.  Calculate error (1 - accuracy) for training and validation sets using `model.score()`.\n",
    "5.  Store errors in `train_nb` and `valid_nb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton for TODO 5\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "train_nb, valid_nb = [], []\n",
    "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # 2. Fit model on training data\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # 3. Calculate error (1 - accuracy)\n",
    "    train_error = 1.0 - model.score(x_train, y_train)\n",
    "    valid_error = 1.0 - model.score(x_valid, y_valid)\n",
    "\n",
    "    # 4. Store results\n",
    "    train_nb.append(train_error)\n",
    "    valid_nb.append(valid_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold, training loss, validation loss\n",
      "    0,          0.05,            0.04\n",
      "    1,          0.02,            0.06\n",
      "    2,          0.04,            0.04\n"
     ]
    }
   ],
   "source": [
    "print(f\"#Fold, training loss, validation loss\")\n",
    "for idx, (train_loss, valid_loss) in enumerate(zip(train_nb, valid_nb)):\n",
    "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO 6**:\n",
    "Answer the following questions in the next cell.  \n",
    "1. What is the the bias-variance trade-off in machine learning?\n",
    "2. How to reduce overfitting and underfitting? \n",
    "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "1. Simple models have high bias and underfit, while complex models have high variance and overfit. The goal is to balance both to minimize generalization error.\n",
    "2. Overfitting can be reduced with regularization, simpler models, or more data. Underfitting can be reduced by increasing model complexity or training longer.\n",
    "3. Naive Bayes trains quickly using probabilistic estimates and strong independence assumptions, while FCNs require iterative gradient-based training and can model complex nonlinear relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
